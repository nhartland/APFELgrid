%% This template can be used to write a paper for
%% Computer Physics Communications using LaTeX.
%% For authors who want to write a computer program description,
%% an example Program Summary is included that only has to be
%% completed and which will give the correct layout in the
%% preprint and the journal.
%% The `elsarticle' style is used and more information on this style
%% can be found at 
%% http://www.elsevier.com/wps/find/authorsview.authors/elsarticle.
%%
%%
\documentclass[preprint,12pt]{elsarticle}

\textwidth=16.0cm \textheight=21.0cm 
\topmargin 0cm \oddsidemargin 0cm 
\setlength{\unitlength}{1mm}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
%% \usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url,hyperref}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}
\biboptions{numbers,sort&compress}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\ba}{\begin{eqnarray}}
\newcommand{\ea}{\end{eqnarray}}

%% This list environment is used for the references in the
%% Program Summary
%%
\newcounter{bla}
\newenvironment{refnummer}{%
\list{[\arabic{bla}]}%
{\usecounter{bla}%
 \setlength{\itemindent}{0pt}%
 \setlength{\topsep}{0pt}%
 \setlength{\itemsep}{0pt}%
 \setlength{\labelsep}{2pt}%
 \setlength{\listparindent}{0pt}%
 \settowidth{\labelwidth}{[9]}%
 \setlength{\leftmargin}{\labelwidth}%
 \addtolength{\leftmargin}{\labelsep}%
 \setlength{\rightmargin}{0pt}}}
 {\endlist}

 \journal{Computer Physics Communications}

 
 
 \begin{document}
 
 \begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{{\tt APFELgrid}: a high performance tool for parton density determinations}
\tnotetext[mytitlenote]{Preprints: OUTP-16-09P, CERN-TH-2016-103}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author[a]{Valerio Bertone}
\author[b]{Stefano Carrazza}
\author[a]{Nathan P. Hartland\corref{author}}

\cortext[author] {Corresponding author.\\\textit{E-mail address:} nathan.hartland@physics.ox.ac.uk}
\address[a]{Rudolf Peierls Centre for Theoretical Physics,\\ 1 Keble Road, University of Oxford, OX1 3NP, Oxford, UK}
\address[b]{Theoretical Physics Department, CERN, Geneva, Switzerland}

\begin{abstract}
  We present a new software package designed to reduce the computational
  burden of hadron collider measurements in Parton Distribution Function (PDF)
  fits. The {\tt APFELgrid} package converts interpolated weight
  tables provided by {\tt APPLgrid} files into a more efficient format
  for PDF fitting by the combination with PDF and
  $\alpha_s$ evolution factors provided by {\tt APFEL}. This
  combination significantly reduces the number of operations required
  to perform the calculation of hadronic observables in PDF fits and
  simplifies the structure of the calculation into a readily optimised
  scalar product. We demonstrate that our technique can lead to a
  substantial speed improvement when compared to existing methods
  without any reduction in numerical accuracy.
  % We present a new method conceived to ease the inclusion of
  % hadronic observables into parton distribution function (PDF)
  % fits. Our technique relies on the the same principles adopted by
  % the existing fast interfaces but implements a set of improvements
  % aimed at optimizing the performance in the context of PDF fits
  % where many iterations and many predictions for each iteration are
  % usually required. The main novelties are the precomputation of the
  % PDF and $\alpha_s$ evolution and the optimisation of the numerical
  % convolution between hard cross sections and PDFs. We demonstrate
  % that our technique can lead to a substantial speed improvement as
  % compared to the existing interfaces.
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
QCD; parton distribution functions; fast predictions.
\end{keyword}

\end{frontmatter}

 \newpage

\noindent {\Large \textbf{Program Summary}}\\
 
\begin{small}
\noindent
{\em Manuscript Title:} {\tt APFELgrid}: a high performance tool for parton density determinations                                      \\
{\em Authors:} V.~Bertone, S.~Carrazza, N.P.~Hartland                                               \\
{\em Program Title:} {\tt APFELgrid}                                          \\
{\em Journal Reference:}                                      \\
  %Leave blank, supplied by Elsevier.
{\em Catalogue identifier:}                                   \\
  %Leave blank, supplied by Elsevier.
{\em Licensing provisions:}   MIT license                                \\
{\em Programming language:}  C++                                 \\
{\em Computer:} PC/Mac                                               \\
  %Computer(s) for which program has been designed.
{\em Operating system:} MacOS/Linux                                       \\
  %Operating system(s) for which program has been designed.
{\em RAM:} varying                                              \\
  %RAM in bytes required to execute program with typical data.
{\em Keywords:} QCD, PDF\\
  % Please give some freely chosen keywords that we can use in a
  % cumulative keyword index.
{\em Classification:}  11.1 General, High Energy Physics and Computing                                       \\
  %Classify using CPC Program Library Subject Index, see (
  % http://cpc.cs.qub.ac.uk/subjectIndex/SUBJECT_index.html)
  %e.g. 4.4 Feynman diagrams, 5 Computer Algebra.
{\em External routines/libraries:}  {\tt APPLgrid}, {\tt APFEL}                          \\
  % Fill in if necessary, otherwise leave out.
{\em Nature of problem:}\\
 Fast computation of hadronic observables under the variation of parton distribution functions.
   \\
{\em Solution method:}\\
  Combination of interpolated weight grids from {\tt APPLgrid} files and evolution factors from {\tt APFEL} into efficient {\tt FastKernel} tables.\\
{\em Running time:} varying\\
   \\
\end{small}

\clearpage

%\tableofcontents

%% main text
\section{Introduction}\label{sec:intro}

Measurements at colliders such as the Tevatron and Large Hadron Collider (LHC) have a unique capacity to shed light upon the internal dynamics 
of the proton and provide constraints upon proton PDFs~\cite{Rojo:2015acz}. However including large hadron collider datasets 
in PDF fits can provide a significant challenge due to the large computational footprint of performing accurate
theoretical predictions over the many iterations required in a fitting procedure. In order to make the fullest use of current 
and future LHC results, efficient strategies for the computation of these observables must therefore be employed. 

The typical Monte Carlo software packages used to perform predictions for 
hadron collider observables cannot be easily deployed in a PDF fit due the processing time 
required to obtain accurate results (usually of the order of a few hours or more per data
point). To overcome such limitations, the typical strategy
adopted for fast cross section prediction relies on the precomputation
of the partonic hard cross sections in such a way that the standard numerical
convolution with any set of PDFs can be reliably approximated by means of
interpolation techniques.

Such interpolation strategies are implemented in the {\tt
  APPLgrid}~\cite{Carli:2010rw} and {\tt
  FastNLO}~\cite{Wobisch:2011ij} projects. For the computation of the
hard cross sections, these packages rely on external codes to which
they are interfaced by means of a suite of functions allowing for the
filling of PDF- and $\alpha_s$-independent look-up tables of
cross section weights. Monte Carlo programs such as {\tt MCFM}~\cite{Campbell:2010ff}
and {\tt NLOJet++}~\cite{Nagy:2003tz} have been interfaced directly to
{\tt APPLgrid}/{\tt FastNLO} and more recently de\-di\-ca\-ted
interfaces to automated general-purpose event
generators have been developed. The {\tt
  aMCfast}~\cite{Bertone:2014zva} and {\tt
  MCgrid}~\cite{DelDebbio:2013kxa} codes can generate interpolation grids in {\tt APPLgrid}/{\tt FastNLO} format 
 by extracting the relevant information from the {\tt
  MadGraph5\_aMC@NLO}~\cite{Alwall:2014hca} and {\tt
  SHERPA}~\cite{Gleisberg:2008ta} event generators respectively.

While these tools have proven to be invaluable in the extraction
of parton densities, the volume of experimental data made available by
LHC collaborations for use in PDF fits is already stretching the
capabilities of the typical fitting technology. A standard
global PDF fit may now include thousands of hadronic data points for which
predictions have to be computed thousands of times during the
minimisation process. As a consequence, performing these predictions
using the standard interpolating tools, $i.e.$
{\tt APPLgrid} and {\tt FastNLO}, starts to become prohibitively time-consuming.
For this reason a high-performance tool tailored specifically to the requirements of PDF analysis becomes increasingly
important.

The {\tt FastKernel} method was developed to
address this problem in the context of the NNPDF global analyses~\cite{Ball:2014uwa}. This method differs from the standard procedure
\`{a} la {\tt APPLgrid} or {\tt FastNLO} in that it maximises the
amount of information that is precomputed prior to fitting so as to
minimise the amount of operations required during the
fit. More specifically, the {\tt FastKernel} method relies on the
combination of precomputed hard cross sections with DGLAP evolution
kernels into a single look-up table, here called
a {\tt FastKernel} ({\tt FK}) table. In this way the prediction
for a given hadronic observable can be obtained by performing a simple matrix
product between the respective {\tt FK} table and PDFs evaluated directly at the fitting scale.

In this paper we present the {\tt APFELgrid} package, a public
implementation of the {\tt FastKernel} method in which the hard
partonic cross sections provided in an {\tt APPLgrid} look-up table
are combined with the DGLAP evolution kernels provided by the {\tt
  APFEL}
package~\cite{Bertone:2013vaa}.

This paper proceeds as follows. In
Sect.~\ref{sec:FastKernel} we present the technical details of the
implementation of the {\tt FastKernel} method. This is followed in
Sect.~\ref{sec:benchmark} by a performance benchmark of the
{\tt APFELgrid} library and resulting {\tt FK} tables. Finally, in
Sect.~\ref{sec:conclusion} we summarise the results discussed in this
work.

\section{Interpolation tools for collider observables}\label{sec:FastKernel}

Hadron collider observables are typically computed in QCD by means of a double
convolution of parton densities with a hard scattering
cross section. Consider for example the calculation of a general
cross section $pp\to X$ with a set of PDFs $\{f\}$:
\begin{equation}\label{eq:hadconv}
  \sigma_{pp\to X} =
  \sum_{s}\sum_{p} \int dx_1\,dx_2\,
  \hat{\sigma}^{(p)(s)}\,\alpha_s^{p+p_{\rm LO}}(Q^2) \,F^{(s)}(x_1,x_2, Q^2)\,,
\end{equation}
where $Q^2$ is the typical hard scale of the process, the index $s$
sums over the active partonic subprocesses in the calculation, $p$ sums over
the perturbative orders used in the expansion, $p_{\rm
  LO}$ is the leading-order power of $\alpha_s$ for the process and
$\hat{\sigma}^{(p)(s)}$ is the N$^p$LO contribution to the
cross section for the partonic subprocess scattering $(s)\to
X$. $F^{(s)}$ represents the subprocess parton density:
\begin{equation}\label{eq:APPLsubproc}
  F^{(s)}(x_1,x_2, Q^2) =\sum_{i,j} C^{(s)}_{ij} 
  f_i(x_{1},Q^2)f_j(x_{2},Q^2)\,,
\end{equation}
where the $C^{(s)}_{ij}$ matrix enumerates the combinations of PDFs
contributing to the $s$-th subprocess.  The central observation of
tools such as {\tt APPLgrid} and {\tt FastNLO} is that the PDF and
$\alpha_S$ dependence may be factorised out of the convolution via
expansion over a set of interpolating functions, spanning $Q^2$ and
the two values of parton-$x$. For example one may represent the
subprocess PDFs and $\alpha_S$ in terms of Lagrange basis
polynomials $\mathcal{I}_\tau(Q^2)$, $\mathcal{I}_\alpha(x_1)$ and
$\mathcal{I}_\beta(x_2)$ as:
\begin{equation}\label{eq:interpolation}
\begin{array}{c}
\displaystyle \alpha_s^{p+p_{\rm LO}}(Q^2) \,F^{(s)}(x_1,x_2, Q^2)
  =\\
\\
\displaystyle \sum_{\alpha,\beta,\tau} \alpha_s^{p+p_{\rm LO}}(Q_\tau^2)
  \,F^{(s)}_{\alpha\beta , \tau}
  \,\mathcal{I}_\tau(Q^2)\,\mathcal{I}_\alpha(x_1)
  \,\mathcal{I}_\beta(x_2),
\end{array}
\end{equation}
where we use the shorthand $F^{(s)}_{\alpha\beta ,\tau} =
F^{(s)}(x_\alpha, x_\beta,Q_\tau^2)$.  Using these expressions in the
double convolution of Eq.~(\ref{eq:hadconv}) one can finally obtain an
expression for the desired cross section which depends upon the
subprocess PDFs only through a simple product:

\begin{equation} \label{eq:applconv}
  \sigma_{pp\to X} = \sum_p \sum_{s}\sum_{\alpha,\beta,\tau} 
  \alpha_s^{p+p_{\rm LO}}(Q^2_\tau)W_{\alpha\beta,\tau}^{(p)(s)} \, F_{\alpha\beta,\tau}^{(s)}\,,
\end{equation}
where
\begin{equation} \label{eq:applgrid}
  W_{\alpha\beta,\tau}^{(p)(s)} = \mathcal{I}_\tau(Q^2)\int dx_1\,dx_2\,
  \hat{\sigma}^{(p)(s)}\,\mathcal{I}_\alpha(x_1)
  \,\mathcal{I}_\beta(x_2)\,,
\end{equation}
consists of the convolution of the hard cross section with the
interpolating polynomials. This information may be stored in a
precomputed look-up table.  The final expression for the cross section
in Eq.~(\ref{eq:applconv}) is therefore a considerably simpler task to perform
inside a fit than the direct evaluation of the double convolution.

\subsection{The FastKernel method}

A number of tools ($e.g.$ {\tt
  APFEL},
{\tt HOPPET}~\cite{Salam:2008qg} and {\tt QCDNUM}~\cite{Botje:2010ay}) 
are available which perform PDF evolution via an analogous
interpolation procedure. In such a way PDFs at a general scale $Q_\tau$ may be
expressed as a product of PDFs at some initial fitting scale $Q_0$ and an
\textit{evolution operator} obtained by the solution of the DGLAP equation.
\begin{equation}\label{eq:fastPDFfinal_recalled}
  f_i(x_{\alpha},Q^2_\tau) = \sum_{k}
  \sum_\beta A^\tau_{\alpha\beta, ik}\;
  f_k(x_\beta,Q^2_0)\,, 
\end{equation}
where latin indices run over PDF flavour, greek indices run over points in an initial-scale interpolating $x$-grid and
the evolution operator $A$ may be accessed directly in the {\tt APFEL} package. Given this operator, we may replace the
(general-scale) PDFs used in the subprocess parton density
Eq.~(\ref{eq:APPLsubproc}) with their equivalent expressions evaluated at the fitting scale as
\begin{equation}\label{eq:FK1}
\begin{array}{rcl}
F^{(s)}_{\alpha\beta,\tau} &=&  \displaystyle \sum_{i,j} \sum_{k,l}
                               \sum_{\delta,\gamma} C^{(s)}_{ij}
                               \left[  A^\tau_{\alpha\delta ik}\;
                               f_k(x_\delta,Q^2_0) A^\tau_{\beta\gamma
                               jl}\; f_l(x_\gamma,Q^2_0) \right]\;\;\;
  \\
\\
&=& \displaystyle \sum_{k,l}\sum_{\delta,\gamma}
\widetilde{C}^{(s),\tau}_{kl,\alpha\beta\gamma\delta}
f_k(x_\delta,Q^2_0) f_l(x_\gamma,Q^2_0)\,,
\end{array}
\end{equation}
with the object
\begin{equation}
  \widetilde{C}^{(s),\tau}_{kl,\alpha\beta\gamma\delta} =
  \sum_{i,j} C^{(s)}_{ij} A^\tau_{\alpha\delta ik}
  A^\tau_{\beta\gamma jl}\,,
\end{equation}
combining the operations of subprocess density construction and PDF
evolution. Going further and using the expression for
subprocess parton densities in Eq.~(\ref{eq:FK1}) in the full
cross section calculation we obtain
\begin{equation}
\sigma_{pp\to X} = \sum_{k,l}\sum_{\delta,\gamma}\sum_p
\sum_{s} \sum_{\alpha,\beta}
\sum_{\tau} 
\alpha_s^{p+p_{\rm LO}}(Q^2_\tau)W_{\alpha\beta,\tau}^{(p)(s)} \widetilde{C}^{(s),\tau}_{kl,\alpha\beta\gamma\delta}
f_k(x_\delta,Q^2_0) f_l(x_\gamma,Q^2_0)\,.
\end{equation}
Performing some further contractions it is possible to obtain an
extremely compact expression for the calculation of the cross section
in question, in terms of only the initial-scale PDFs and summing only
over the initial scale interpolating $x$-grid and the incoming parton
flavours:
\begin{equation}\label{eq:FK}
  \sigma _{pp\to X} = \sum_{k,l}\sum_{\delta,\gamma} 
  \widetilde{W}_{kl,\delta\gamma} \,f_k(x_\delta,Q^2_0) f_l(x_\gamma,Q^2_0),
\end{equation}
where the object:
\begin{equation}\label{eq:FKTable}
  \widetilde{W}_{kl,\delta\gamma} = \sum_p\sum_{s}\sum_{\alpha,\beta} \sum_{\tau}
\alpha_s^{p+p_{\rm LO}}(Q^2_\tau)  W_{\alpha\beta,\tau}^{(p)(s)} \widetilde{C}^{(s),\tau}_{kl,\alpha\beta\gamma\delta}
\end{equation}
is referred to here as an {\tt FK} table, and combines the information
stored in {\tt APPLgrid}-style interpolated weight grids with
analogously interpolated DGLAP evolution operators. This combination
enables for a maximally efficient expression for the
calculation of observables at hadron colliders under PDF variation, without invoking
any additional approximation. 

\subsection{Features and limitations of {\tt FK} tables}

The {\tt FK} product of Eq.~(\ref{eq:FK}) differs with respect to the
product in Eq.~(\ref{eq:applconv}) in several notable ways. Firstly
the typical {\tt APPLgrid} or {\tt FastNLO} products use as input PDFs at a
general scale, requiring that PDF evolution {\it e.g.}
Eq.~(\ref{eq:fastPDFfinal_recalled}) be performed for every variation of the PDFs
during the fit. In the {\tt FK} product
this evolution is pre-cached at the stage of {\tt FK} table
generation, requiring only initial-scale PDFs at the time of fitting. 
This pre-caching of the evolution also removes the need to
 sum over hard scale and perturbative order during the
fit, further reducing the number of operations required. As the {\tt FK}
product acts directly at the fitting scale, it benefits from the typically reduced number of active
partonic modes, with the sum over flavours in
Eq.~(\ref{eq:FK}) being limited to those directly parametrised in the
fit. Having reduced the calculation to such a simple form, it is also
straightforward to apply standard 
computational tools such as multi-threading through e.g OpenMP or
Single Instruction Multiple Data (SIMD) operations such as SSE or AVX to
further reduce computational expense. 

While these features provide significant performance enhancements, the
{\tt FK} table format is not suitable as a complete replacement for
tools such as {\tt APPLgrid}. The precomputation of the PDF evolution
necessarily means that all theory parameters such as perturbative
order, strong coupling and factorization/renormalization scales are
inextricably embedded in each {\tt FK} table. In order to perform PDF
fits including variations of these parameters, multiple {\tt FK}
tables must be computed, each with different theory settings. While
performing such a re-calculation directly from Monte Carlo codes would
be exceptionally time consuming, the data representation in {\tt
  APPLgrid} files allows for an efficient (re)-combination with varying
theory parameters.

\section{Performance benchmarks}
\label{sec:benchmark}

We shall now examine the performance differences between
the two expressions for fast interpolated cross section prediction
Eq.~(\ref{eq:applconv}) ({\tt APPLgrid}) and Eq.~(\ref{eq:FK}) ({\tt
  FK}).
%
In order to provide a comprehensive benchmark, we consider here a wide range of
processes including LHC and Tevatron electroweak vector boson
production
measurements~\cite{Aaij:2012mda,Aaij:2012vn,Chatrchyan:2013mza,Chatrchyan:2013uja,Chatrchyan:2012xt,Aad:2013iua,Aad:2011fp,Aad:2011dm,Aaltonen:2010zza},
$t\bar{t}$ total
cross sections~\cite{ATLAS:2012aa, Chatrchyan:2013faa,Chatrchyan:2012bra,Chatrchyan:2012ria},
double-differential Drell-Yan
cross sections~\cite{Chatrchyan:2013tia,CMS:2014jea} and inclusive jet
data~\cite{Chatrchyan:2012bja,Aad:2011fc,Aad:2013lpa,Abazov:2007jy}. Predictions
are performed over a wide range of kinematics, for a total of 52
source {\tt APPLgrid} files corresponding to the majority of available
LHC and Tevatron datasets applicable to PDF determination.
While the source {\tt APPLgrid} files have varying grid densities in
$x$ and $Q^2$, for the purposes of comparison the corresponding {\tt
  FK} tables are produced consistently with 30 points in $x$, and at
an initial scale below the charm threshold, therefore with seven
active partonic species. These settings are chosen so as to provide a realistic
comparison, in a production environment the density and distribution of the
$x$-grid may be adjusted to match interpolation accuracy requirements.  For these comparisons the {\tt FK}
table is stored as double-precision in memory for table generation and in
single-precision for the purposes of computing the {\tt FK} product.

In Fig.~\ref{fig:timings} we compare the average time taken per
datapoint for the {\tt FK} and {\tt APPLgrid} calculations, for all of
the 52 tables. We show timings for the {\tt FK} calculation in four
different configurations: AVX-OpenMP 2x (2 CPU cores), AVX, SSE3 and
the standard double precision product. Due to the inherent structural
differences between the {\tt FK} and {\tt APPLgrid} procedures,
results from the {\tt FK} calculation are systematically faster than
those from {\tt APPLgrid}. In particular, when comparing {\tt
  FK} AVX-OpenMP 2x to {\tt APPLgrid} timings we obtain minimally a factor of
ten improvement in speed for electroweak vector boson production and a
maximum factor of 2000 improvement in predictions for inclusive jet
data. Across all processes and kinematic regions we observe
significant performance improvements from using the {\tt FK}
calculation even without the use of SIMD or multi-threading.

While sheer computational speed is typically the primary consideration
when com\-pu\-ting observables in a PDF fit, other factors such as
table size in the filesystem and memory, along with the computational
cost of pre-computing {\tt FK} tables must be considered. Indeed, the
computation of the {\tt FK} table in Eq.~(\ref{eq:FKTable}) requires a
great deal of operations which can be time consuming, particularly in
the case of source {\tt APPLgrid} files with very high interpolation
precision.

In Fig.~\ref{fig:performance} we examine the {\tt FK} table generation
time with {\tt APFELgrid}, {\tt FK} table file size and memory usage
of the {\tt FK} tables arising from the same source {\tt APPLgrid}
files as discussed in Fig.~\ref{fig:timings}. When examining the
table generation time per point, we observe timings from a few
milliseconds to 3.5 minutes per point, with differences arising from
the varying grid densities used in the input {\tt APPLgrid} files.  In
terms of the grid size on disk, {\tt FK} tables are typically larger
than their corresponding {\tt APPLgrid} files, primarily as the {\tt
  FK} file format is encoded in plain text for compatibility whereas
{\tt APPLgrid} files are expressed in binary as {\tt ROOT}
files. However when measuring the in-memory resident set size used by
the two procedures, the amount of system memory used by {\tt
  FK} tables is systematically less than {\tt APPLgrid} files for all
processes considered here by at least 75\%. Note that this effect is
in part due to the differing precisions of the default representations.

%In conclusion, the great performance benefits obtained with {\tt
%APFELgrid} endorse the importance of this tool not only in PDF fits
%but also in any application requiring a large number of convolutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  \includegraphics[scale=0.6]{plots/t0a}
\caption{\small Performance comparisons between {\tt APFELgrid} with
  AVX-OpenMP 2x, AVX, SSE3, double precision convolution and {\tt
    APPLgrid} convolution time per point and process.}
\label{fig:timings}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering                                      
  \includegraphics[scale=0.6]{plots/t0b}
\caption{\small {\tt APFELgrid} generation time per point and
  comparison to {\tt APPLgrid} for grid size on disk and resident set size
  (RSS).}
\label{fig:performance}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}\label{sec:conclusion}

In this work we have demonstrated that by employing the so-called {\tt
  FastKernel} method, it is possible to convert an {\tt APPLgrid}
weight table into a derived format, referred to as an {\tt FK} table,
including the effects of PDF and $\alpha_s$ evolution. This procedure has been
implemented in the {\tt APFELgrid} package, supplied as a set of {\tt
  C++} routines designed to supplement the PDF evolution library {\tt
  APFEL} with {\tt FK} table generation capabilities.
%The plugin has a very light interface for performing the {\tt FK} combination,
%  and relies on the native {\tt APFEL} API for setting the parameters of the PDF evolution,
%  including the structure of the initial scale $x$ grid.
The {\tt APFELgrid} package allows one to obtain a computationally
efficient expression for the calculation of hadronic cross sections,
in terms of only the initial-scale PDFs and summing only over the
initial scale interpolating $x$-grid and the incoming parton
flavours. The simple structure of the resulting
product makes {\tt FK} tables particularly suitable for the
efficient use of computational tools such as {\tt SIMD} and {\tt
  OpenMP}.

We have shown that in several practical examples the numerical
evaluation of an {\tt FK} product is considerably faster than the
corresponding {\tt APPLgrid} product, even in the case where neither
SIMD or multi-threading are applied. {\tt FK} tables are supplied in a
simple plain-text format in order to simplify the construction of user
interfaces, and therefore are typically larger than 
corresponding {\tt APPLgrids}. However we have shown that the
in-memory resident set sizes occupied by {\tt FK} tables are typically
smaller than those required by {\tt APPLgrids}, in our examples by at least 75\%.

The substantial speed improvement of {\tt FK} tables with respect to
{\tt APPLgrid} in association with the reduction in memory footprint
makes the {\tt APFELgrid} code a valuable tool for modern PDF fits
including large collider datasets.
%We stress that {\tt APFELgrid} is not
%a replacement for {\tt APPLgrid}, on which it depends, but it is
%rather a specialised tool conceived to ease the inclusion of hadronic
%data into PDF fits. In fact, the price of a better performance is the
%fact that, by themselves, {\tt FKtables} cannot perform any parameter
%variation other than that of PDFs. For example, variations of
%$\alpha_s$ can be performed only by re-computing the {\tt FKtables}.

The {\tt APFELgrid} package and associated documentation are publicly
available from the webpage:
\begin{center}
\url{https://github.com/nhartland/APFELgrid}
\end{center}

\section*{Acknowledgements}

The authors would like to thank members of the NNPDF Collaboration for their support and
motivation for this work; particularly Juan Rojo, Luigi del Debbio and Alberto Guffanti. We would also like
to thank Mark Sutton for helpful comments on the paper. V.~B. and N.~H. are
supported by an European Research Council Starting Grant ``PDF4BSM''.
S.~C. is supported by the HICCUP ERC Consolidator grant (614577).


%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with bibTeX database:
\section*{References}
\bibliographystyle{elsarticle-num}
\bibliography{paper}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-num.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}

\end{document}

%%
%% End of file 
